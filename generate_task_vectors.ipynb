{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import RotatedMNIST\n",
    "\n",
    "# Generate RotatedMNIST pseudo-tasks (e.g., 7 tasks with different rotations)\n",
    "rotated_benchmark = RotatedMNIST(n_experiences=7, seed=1234)\n",
    "\n",
    "# Print rotation details for each experience\n",
    "for i, experience in enumerate(rotated_benchmark.train_stream):\n",
    "    print(f\"Experience {i+1} rotation: {experience.task_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "\n",
    "# Standard MNIST training\n",
    "split_mnist = SplitMNIST(n_experiences=1, seed=1234)\n",
    "model_base = SimpleMLP(num_classes=10).to(device)\n",
    "\n",
    "trainer = Naive(\n",
    "    model_base,\n",
    "    optimizer=torch.optim.SGD(model_base.parameters(), lr=0.01),\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    train_mb_size=128,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Pretrain on MNIST\n",
    "trainer.train(split_mnist.train_stream[0], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = []  # To store pre-populated task vectors\n",
    "\n",
    "for experience in rotated_benchmark.train_stream:\n",
    "    model_tuned = SimpleMLP(num_classes=10).to(device)\n",
    "    model_tuned.load_state_dict(model_base.state_dict())  # Start from base model\n",
    "\n",
    "    # Fine-tune on rotated task\n",
    "    trainer = Naive(\n",
    "        model_tuned,\n",
    "        optimizer=torch.optim.SGD(model_tuned.parameters(), lr=0.01),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        train_mb_size=128,\n",
    "        device=device\n",
    "    )\n",
    "    trainer.train(experience, epochs=5)\n",
    "\n",
    "    # Compute task vector\n",
    "    task_vector = []\n",
    "    for p_base, p_tuned in zip(model_base.parameters(), model_tuned.parameters()):\n",
    "        task_vector.append((p_tuned.data - p_base.data).detach().cpu().numpy())\n",
    "    task_vector = np.concatenate([p.flatten() for p in task_vector])\n",
    "\n",
    "    # Store the task vector\n",
    "    task_vectors.append(task_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save pre-populated task vectors to a file\n",
    "with open(\"rotated_task_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(task_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-populated task vectors\n",
    "with open(\"rotated_task_vectors.pkl\", \"rb\") as f:\n",
    "    task_vectors_active = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
