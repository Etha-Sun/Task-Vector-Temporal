{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel_schapiro/anaconda3/envs/py3.9.13/lib/python3.9/site-packages/avalanche/training/templates/base.py:468: PositionalArgumentsDeprecatedWarning: Avalanche is transitioning to strategy constructors that accept named (keyword) arguments only. This is done to ensure that there is no confusion regarding the meaning of each argument (strategies can have many arguments). Your are passing 3 positional arguments to the Naive.__init__ method. Consider passing them as names arguments. The ability to pass positional arguments will be removed in the future.\n",
      "  warnings.warn(error_str, category=PositionalArgumentsDeprecatedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Training Joint Baseline Model ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:41<00:00,  2.12it/s]\n",
      "Epoch 0 ended.\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:38<00:00,  2.14it/s]\n",
      "Epoch 0 ended.\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:40<00:00,  2.13it/s]\n",
      "Epoch 0 ended.\n",
      "-- >> End of training phase << --\n",
      "\n",
      "Joint Baseline Model Training Completed.\n",
      "\n",
      "### Training on Task 1 (Rotation: 115°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:50<00:00,  2.03it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8127\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7831\n",
      "-- >> End of training phase << --\n",
      "Task vector 1 is NOT in span. Adding to active set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_347500/1906438261.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_slice = torch.tensor(mask[offset:offset + numel]).view(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Regret = -1.4023, Average Regret = -0.4674\n",
      "\n",
      "### Training on Task 2 (Rotation: 145°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:55<00:00,  2.00it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task001 = 0.5820\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task001 = 0.8244\n",
      "-- >> End of training phase << --\n",
      "Task vector 2 is NOT in span. Adding to active set.\n",
      "Task 2: Regret = -0.2985, Average Regret = -0.5669\n",
      "\n",
      "### Training on Task 3 (Rotation: 175°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [03:52<00:00,  2.02it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task002 = 0.5881\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task002 = 0.8229\n",
      "-- >> End of training phase << --\n",
      "Task vector 3 is NOT in span. Adding to active set.\n",
      "Task 3: Regret = -0.0018, Average Regret = -0.5675\n",
      "\n",
      "### Average Regret Across All Tasks: -0.5675 ###\n",
      "\n",
      "### Evaluating Model ###\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 10000/10000 [00:36<00:00, 276.23it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "-- Starting eval on experience 1 (Task 1) from test stream --\n",
      "100%|██████████| 10000/10000 [00:35<00:00, 281.73it/s]\n",
      "> Eval on experience 1 (Task 1) from test stream ended.\n",
      "-- Starting eval on experience 2 (Task 2) from test stream --\n",
      "100%|██████████| 10000/10000 [00:34<00:00, 289.02it/s]\n",
      "> Eval on experience 2 (Task 2) from test stream ended.\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task002 = 0.7142\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from avalanche.benchmarks import RotatedMNIST\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy.linalg import lstsq\n",
    "import pickle\n",
    "\n",
    "# Helper function: Load precomputed task vectors\n",
    "def load_task_vectors(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Helper function: Compute task vector\n",
    "def compute_task_vector(pretrained_model, finetuned_model):\n",
    "    task_vector = []\n",
    "    param_shapes = []  # Store parameter shapes\n",
    "    \n",
    "    for p_pre, p_fine in zip(pretrained_model.parameters(), finetuned_model.parameters()):\n",
    "        param_shapes.append(p_pre.shape)  # Save the shape of each parameter\n",
    "        task_vector.append((p_fine.data - p_pre.data).detach().cpu().numpy())\n",
    "    \n",
    "    # Flatten the task vector\n",
    "    flattened_task_vector = np.concatenate([p.flatten() for p in task_vector])\n",
    "    return flattened_task_vector, param_shapes\n",
    "\n",
    "# Helper function: Check if task vector is in the span of active task vectors\n",
    "def is_in_span(vector, span_vectors):\n",
    "    if not span_vectors:\n",
    "        return False, None\n",
    "    span_matrix = np.stack(span_vectors, axis=1)  # Stack vectors as columns\n",
    "    coeffs, residuals, _, _ = lstsq(span_matrix, vector, rcond=None)\n",
    "    in_span = np.allclose(span_matrix @ coeffs, vector, atol=1e-5)\n",
    "    return in_span, coeffs\n",
    "\n",
    "# Class for Localize and Stitch\n",
    "class LocalizeAndStitch:\n",
    "    def __init__(self, model, pretrained_model, task_vectors_active, sparsity=0.01):\n",
    "        self.model = model\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.task_vectors_active = task_vectors_active\n",
    "        self.sparsity = sparsity  # Sparsity level for localization\n",
    "\n",
    "    def localize_task(self, task_vector):\n",
    "        # Sparse mask creation\n",
    "        abs_vector = torch.abs(torch.tensor(task_vector))\n",
    "        k = int(self.sparsity * abs_vector.numel())\n",
    "        topk_indices = abs_vector.topk(k).indices\n",
    "        mask = torch.zeros_like(abs_vector)\n",
    "        mask[topk_indices] = 1\n",
    "        return mask\n",
    "\n",
    "    def apply_task_vector(self, task_vector, mask, param_shapes):\n",
    "        # Apply sparse task vector updates using the mask and unflatten\n",
    "        offset = 0\n",
    "        for p, shape in zip(self.model.parameters(), param_shapes):\n",
    "            numel = np.prod(shape)  # Number of elements in the parameter\n",
    "            # Unflatten task vector slice to match parameter shape\n",
    "            task_slice = torch.tensor(task_vector[offset:offset + numel]).view(shape)\n",
    "            mask_slice = torch.tensor(mask[offset:offset + numel]).view(shape)\n",
    "            # Apply updates with the mask\n",
    "            p.data += (task_slice * mask_slice).to(p.device)\n",
    "            offset += numel\n",
    "\n",
    "\n",
    "# Function: Train joint baseline model\n",
    "def train_joint_baseline(benchmark, device, epochs=5):\n",
    "    print(\"\\n### Training Joint Baseline Model ###\")\n",
    "    joint_model = SimpleMLP(num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(joint_model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(accuracy_metrics(stream=True), loggers=[logger])\n",
    "\n",
    "    trainer = Naive(joint_model, optimizer, criterion, train_mb_size=128, device=device, evaluator=eval_plugin)\n",
    "    \n",
    "    # Train on all tasks\n",
    "    for experience in benchmark.train_stream:\n",
    "        trainer.train(experience, epochs=epochs)\n",
    "    \n",
    "    print(\"\\nJoint Baseline Model Training Completed.\")\n",
    "    return joint_model\n",
    "\n",
    "\n",
    "# Function: Evaluate model loss dynamically for regret computation\n",
    "def evaluate_loss(model, test_stream, criterion, device, task_id):\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        loss_metrics(stream=True),\n",
    "        loggers=[TextLogger(open(\"/dev/null\", \"w\"))]  # Suppress logger output\n",
    "    )\n",
    "\n",
    "    # Use a Naive trainer for evaluation\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    evaluator = Naive(model, optimizer, criterion, device=device, evaluator=eval_plugin)\n",
    "\n",
    "    evaluator.eval(test_stream)\n",
    "\n",
    "    # Retrieve loss dynamically for the current task\n",
    "    task_key = f\"Loss_Stream/eval_phase/test_stream/Task{task_id:03d}\"\n",
    "    eval_results = eval_plugin.get_last_metrics()\n",
    "\n",
    "    if task_key in eval_results:\n",
    "        return eval_results[task_key]\n",
    "    else:\n",
    "        raise ValueError(f\"Loss key {task_key} not found in evaluation results.\")\n",
    "\n",
    "\n",
    "# Main workflow for continual learning\n",
    "def continual_learning_with_localize_and_stitch():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Benchmark: RotatedMNIST for new tasks [115°, 145°, 175°]\n",
    "    rotation_angles = [115, 145, 175]\n",
    "    rotated_benchmark = RotatedMNIST(n_experiences=len(rotation_angles), seed=1234, rotations_list=rotation_angles, return_task_id=True)\n",
    "\n",
    "    joint_model = train_joint_baseline(rotated_benchmark, device, epochs=5)\n",
    "\n",
    "    # Load precomputed task vectors (from [0°, 15°, ..., 90°])\n",
    "    task_vectors_active = load_task_vectors(\"rotated_task_vectors.pkl\")\n",
    "\n",
    "    # Model initialization\n",
    "    model_base = SimpleMLP(num_classes=10).to(device)\n",
    "    optimizer = torch.optim.SGD(model_base.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Pretrained model\n",
    "    model_pretrained = SimpleMLP(num_classes=10).to(device)\n",
    "    model_pretrained.load_state_dict(model_base.state_dict())\n",
    "\n",
    "    # Evaluation plugin\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(epoch=True, stream=True),\n",
    "        loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # Localize and Stitch handler\n",
    "    las_handler = LocalizeAndStitch(model_base, model_pretrained, task_vectors_active)\n",
    "\n",
    "    # Regret calculation\n",
    "    avg_regret = 0\n",
    "\n",
    "    # Training loop over new tasks\n",
    "    for task_id, experience in enumerate(rotated_benchmark.train_stream):\n",
    "        print(f\"\\n### Training on Task {task_id+1} (Rotation: {rotation_angles[task_id]}°) ###\")\n",
    "\n",
    "        # Fine-tune model on current task\n",
    "        model_finetuned = SimpleMLP(num_classes=10).to(device)\n",
    "        model_finetuned.load_state_dict(model_pretrained.state_dict())\n",
    "\n",
    "        trainer = Naive(\n",
    "            model_finetuned,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            train_mb_size=128,\n",
    "            device=device\n",
    "        )\n",
    "        trainer.train(experience, epochs=1)\n",
    "\n",
    "        # Compute the task vector for the current task\n",
    "        task_vector, param_shapes = compute_task_vector(model_pretrained, model_finetuned)\n",
    "\n",
    "        # Check if the task vector is in the span of active task vectors\n",
    "        in_span, coefficients = is_in_span(task_vector, task_vectors_active)\n",
    "        if in_span:\n",
    "            print(f\"Task vector {task_id+1} is in span of active task vectors.\")\n",
    "        else:\n",
    "            print(f\"Task vector {task_id+1} is NOT in span. Adding to active set.\")\n",
    "            task_vectors_active.append(task_vector)\n",
    "\n",
    "        # Localize task vector and apply it to the model\n",
    "        sparse_mask = las_handler.localize_task(task_vector)\n",
    "        las_handler.apply_task_vector(task_vector, sparse_mask, param_shapes)\n",
    "\n",
    "        # Update the pretrained model\n",
    "        model_pretrained.load_state_dict(model_finetuned.state_dict())\n",
    "\n",
    "        # Compute losses for regret\n",
    "        test_stream = rotated_benchmark.test_stream[task_id]\n",
    "        cl_loss = evaluate_loss(model_finetuned, test_stream, criterion, device, task_id)\n",
    "        joint_loss = evaluate_loss(joint_model, test_stream, criterion, device, task_id)\n",
    "\n",
    "        # Compute regret\n",
    "        regret = cl_loss - joint_loss\n",
    "        avg_regret += regret / len(rotated_benchmark.train_stream)\n",
    "        print(f\"Task {task_id+1}: Regret = {regret:.4f}, Average Regret = {avg_regret:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n### Average Regret Across All Tasks: {avg_regret:.4f} ###\")\n",
    "    \n",
    "    # Evaluate on all tasks\n",
    "    print(\"\\n### Evaluating Model ###\")\n",
    "    evaluator = Naive(\n",
    "        model_base,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=device,\n",
    "        evaluator=eval_plugin\n",
    "    )\n",
    "    evaluator.eval(rotated_benchmark.test_stream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    continual_learning_with_localize_and_stitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
