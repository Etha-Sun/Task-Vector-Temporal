{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel_schapiro/anaconda3/envs/py3.9.13/lib/python3.9/site-packages/avalanche/training/templates/base.py:468: PositionalArgumentsDeprecatedWarning: Avalanche is transitioning to strategy constructors that accept named (keyword) arguments only. This is done to ensure that there is no confusion regarding the meaning of each argument (strategies can have many arguments). Your are passing 3 positional arguments to the Naive.__init__ method. Consider passing them as names arguments. The ability to pass positional arguments will be removed in the future.\n",
      "  warnings.warn(error_str, category=PositionalArgumentsDeprecatedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Training on Task 1 (Rotation: 115°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [04:11<00:00,  1.87it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8249\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7804\n",
      "-- >> End of training phase << --\n",
      "Task vector 1 is NOT in span. Adding to active set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_347500/3770900792.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_slice = torch.tensor(mask[offset:offset + numel]).view(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Training on Task 2 (Rotation: 145°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [04:08<00:00,  1.89it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.7832\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7656\n",
      "-- >> End of training phase << --\n",
      "Task vector 2 is NOT in span. Adding to active set.\n",
      "\n",
      "### Training on Task 3 (Rotation: 175°) ###\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 469/469 [04:08<00:00,  1.89it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.5685\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8268\n",
      "-- >> End of training phase << --\n",
      "Task vector 3 is NOT in span. Adding to active set.\n",
      "\n",
      "### Evaluating Model ###\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 10000/10000 [00:35<00:00, 283.30it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 10000/10000 [00:38<00:00, 260.65it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 10000/10000 [00:37<00:00, 268.81it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.6419\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from avalanche.benchmarks import RotatedMNIST\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.evaluation.metrics import accuracy_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "# Helper function: Load precomputed task vectors\n",
    "def load_task_vectors(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Helper function: Compute task vector\n",
    "def compute_task_vector(pretrained_model, finetuned_model):\n",
    "    task_vector = []\n",
    "    param_shapes = []  # Store parameter shapes\n",
    "    \n",
    "    for p_pre, p_fine in zip(pretrained_model.parameters(), finetuned_model.parameters()):\n",
    "        param_shapes.append(p_pre.shape)  # Save the shape of each parameter\n",
    "        task_vector.append((p_fine.data - p_pre.data).detach().cpu().numpy())\n",
    "    \n",
    "    # Flatten the task vector\n",
    "    flattened_task_vector = np.concatenate([p.flatten() for p in task_vector])\n",
    "    return flattened_task_vector, param_shapes\n",
    "\n",
    "# Helper function: Check if task vector is in the span of active task vectors\n",
    "def is_in_span(vector, span_vectors):\n",
    "    if not span_vectors:\n",
    "        return False, None\n",
    "    span_matrix = np.stack(span_vectors, axis=1)  # Stack vectors as columns\n",
    "    coeffs, residuals, _, _ = lstsq(span_matrix, vector, rcond=None)\n",
    "    in_span = np.allclose(span_matrix @ coeffs, vector, atol=1e-5)\n",
    "    return in_span, coeffs\n",
    "\n",
    "# Class for Localize and Stitch\n",
    "class LocalizeAndStitch:\n",
    "    def __init__(self, model, pretrained_model, task_vectors_active, sparsity=0.01):\n",
    "        self.model = model\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.task_vectors_active = task_vectors_active\n",
    "        self.sparsity = sparsity  # Sparsity level for localization\n",
    "\n",
    "    def localize_task(self, task_vector):\n",
    "        # Sparse mask creation\n",
    "        abs_vector = torch.abs(torch.tensor(task_vector))\n",
    "        k = int(self.sparsity * abs_vector.numel())\n",
    "        topk_indices = abs_vector.topk(k).indices\n",
    "        mask = torch.zeros_like(abs_vector)\n",
    "        mask[topk_indices] = 1\n",
    "        return mask\n",
    "\n",
    "    def apply_task_vector(self, task_vector, mask, param_shapes):\n",
    "        # Apply sparse task vector updates using the mask and unflatten\n",
    "        offset = 0\n",
    "        for p, shape in zip(self.model.parameters(), param_shapes):\n",
    "            numel = np.prod(shape)  # Number of elements in the parameter\n",
    "            # Unflatten task vector slice to match parameter shape\n",
    "            task_slice = torch.tensor(task_vector[offset:offset + numel]).view(shape)\n",
    "            mask_slice = torch.tensor(mask[offset:offset + numel]).view(shape)\n",
    "            # Apply updates with the mask\n",
    "            p.data += (task_slice * mask_slice).to(p.device)\n",
    "            offset += numel\n",
    "\n",
    "# Main workflow for continual learning\n",
    "def continual_learning_with_localize_and_stitch():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Benchmark: RotatedMNIST for new tasks [115°, 145°, 175°]\n",
    "    rotation_angles = [115, 145, 175]\n",
    "    rotated_benchmark = RotatedMNIST(n_experiences=len(rotation_angles), seed=1234)\n",
    "\n",
    "    # Load precomputed task vectors (from [0°, 15°, ..., 90°])\n",
    "    task_vectors_active = load_task_vectors(\"rotated_task_vectors.pkl\")\n",
    "\n",
    "    # Model initialization\n",
    "    model_base = SimpleMLP(num_classes=10).to(device)\n",
    "    optimizer = torch.optim.SGD(model_base.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Pretrained model\n",
    "    model_pretrained = SimpleMLP(num_classes=10).to(device)\n",
    "    model_pretrained.load_state_dict(model_base.state_dict())\n",
    "\n",
    "    # Evaluation plugin\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(epoch=True, stream=True),\n",
    "        loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # Localize and Stitch handler\n",
    "    las_handler = LocalizeAndStitch(model_base, model_pretrained, task_vectors_active)\n",
    "\n",
    "    # Training loop over new tasks\n",
    "    for task_id, experience in enumerate(rotated_benchmark.train_stream):\n",
    "        print(f\"\\n### Training on Task {task_id+1} (Rotation: {rotation_angles[task_id]}°) ###\")\n",
    "\n",
    "        # Fine-tune model on current task\n",
    "        model_finetuned = SimpleMLP(num_classes=10).to(device)\n",
    "        model_finetuned.load_state_dict(model_pretrained.state_dict())\n",
    "\n",
    "        trainer = Naive(\n",
    "            model_finetuned,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            train_mb_size=128,\n",
    "            device=device\n",
    "        )\n",
    "        trainer.train(experience, epochs=1)\n",
    "\n",
    "        # Compute the task vector for the current task\n",
    "        task_vector, param_shapes = compute_task_vector(model_pretrained, model_finetuned)\n",
    "\n",
    "        # Check if the task vector is in the span of active task vectors\n",
    "        in_span, coefficients = is_in_span(task_vector, task_vectors_active)\n",
    "        if in_span:\n",
    "            print(f\"Task vector {task_id+1} is in span of active task vectors.\")\n",
    "        else:\n",
    "            print(f\"Task vector {task_id+1} is NOT in span. Adding to active set.\")\n",
    "            task_vectors_active.append(task_vector)\n",
    "\n",
    "        # Localize task vector and apply it to the model\n",
    "        sparse_mask = las_handler.localize_task(task_vector, param_shapes)\n",
    "        las_handler.apply_task_vector(task_vector, sparse_mask)\n",
    "\n",
    "        # Update the pretrained model\n",
    "        model_pretrained.load_state_dict(model_finetuned.state_dict())\n",
    "\n",
    "    # Evaluate on all tasks\n",
    "    print(\"\\n### Evaluating Model ###\")\n",
    "    evaluator = Naive(\n",
    "        model_base,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=device,\n",
    "        evaluator=eval_plugin\n",
    "    )\n",
    "    evaluator.eval(rotated_benchmark.test_stream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    continual_learning_with_localize_and_stitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
